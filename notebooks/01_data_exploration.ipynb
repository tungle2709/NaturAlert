{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration - Disaster Early Warning System\n",
    "\n",
    "This notebook explores the available weather datasets to understand:\n",
    "- Data structure and quality\n",
    "- Missing values and data distributions\n",
    "- Weather patterns and extreme conditions\n",
    "- Correlations between weather variables\n",
    "\n",
    "## Datasets Available:\n",
    "1. **GlobalWeatherRepository.csv** - Global weather data for 195+ countries\n",
    "2. **weather_classification_data.csv** - Labeled weather types (13,201 rows)\n",
    "3. **rain_prediction_2500observations.csv** - Binary rain prediction dataset\n",
    "4. **weather_data.csv** - Large-scale weather observations\n",
    "5. **top100cities_weather_data.csv** - Weather data for top 100 cities\n",
    "6. **seattle-weather.csv** - Historical Seattle weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "dataset_dir = Path('../dataset')\n",
    "datasets = {}\n",
    "\n",
    "# Load all CSV datasets\n",
    "try:\n",
    "    datasets['global'] = pd.read_csv(dataset_dir / 'GlobalWeatherRepository.csv')\n",
    "    print(f\"✓ Global Weather: {datasets['global'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Global Weather: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['classification'] = pd.read_csv(dataset_dir / 'weather_classification_data.csv')\n",
    "    print(f\"✓ Weather Classification: {datasets['classification'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Weather Classification: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['rain_prediction'] = pd.read_csv(dataset_dir / 'rain_prediction_2500observations.csv')\n",
    "    print(f\"✓ Rain Prediction: {datasets['rain_prediction'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Rain Prediction: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['weather_large'] = pd.read_csv(dataset_dir / 'weather_data.csv')\n",
    "    print(f\"✓ Weather Large: {datasets['weather_large'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Weather Large: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['top_cities'] = pd.read_csv(dataset_dir / 'top100cities_weather_data.csv')\n",
    "    print(f\"✓ Top Cities: {datasets['top_cities'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Top Cities: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['seattle'] = pd.read_csv(dataset_dir / 'seattle-weather.csv')\n",
    "    print(f\"✓ Seattle Weather: {datasets['seattle'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Seattle Weather: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(datasets)} datasets successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine structure of each dataset\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"{i+1:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values for each dataset\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, df) in enumerate(datasets.items()):\n",
    "    if i < len(axes):\n",
    "        missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "        missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "        \n",
    "        if len(missing_pct) > 0:\n",
    "            missing_pct.plot(kind='bar', ax=axes[i], color='coral')\n",
    "            axes[i].set_title(f'{name.title()} - Missing Values %')\n",
    "            axes[i].set_ylabel('Missing %')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f'{name.title()}\\nNo Missing Values', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes,\n",
    "                        fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "            axes[i].set_xticks([])\n",
    "            axes[i].set_yticks([])\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(datasets), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed missing value statistics\n",
    "print(\"\\nDetailed Missing Value Analysis:\")\n",
    "print(\"=\"*60)\n",
    "for name, df in datasets.items():\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_pct = (missing_count / total_cells) * 100\n",
    "    print(f\"{name.title():20s}: {missing_count:6d} missing ({missing_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weather Variable Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on weather classification dataset for detailed analysis\n",
    "if 'classification' in datasets:\n",
    "    df_weather = datasets['classification']\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df_weather.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"Numeric columns: {numeric_cols}\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:9]):\n",
    "        if i < len(axes):\n",
    "            df_weather[col].hist(bins=30, ax=axes[i], alpha=0.7, color='skyblue')\n",
    "            axes[i].set_title(f'{col} Distribution')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_val = df_weather[col].mean()\n",
    "            std_val = df_weather[col].std()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.1f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df_weather[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extreme Weather Conditions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze extreme weather conditions across datasets\n",
    "def identify_extreme_conditions(df, dataset_name):\n",
    "    \"\"\"Identify extreme weather conditions in a dataset\"\"\"\n",
    "    print(f\"\\n{dataset_name.upper()} - Extreme Conditions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Try to identify relevant columns (case-insensitive)\n",
    "    cols = df.columns.str.lower()\n",
    "    \n",
    "    # Temperature extremes\n",
    "    temp_cols = [col for col in df.columns if any(x in col.lower() for x in ['temp', 'celsius', 'fahrenheit'])]\n",
    "    if temp_cols:\n",
    "        temp_col = temp_cols[0]\n",
    "        temp_data = pd.to_numeric(df[temp_col], errors='coerce')\n",
    "        print(f\"Temperature ({temp_col}):\")\n",
    "        print(f\"  Extreme Cold: < {temp_data.quantile(0.05):.1f} ({(temp_data < temp_data.quantile(0.05)).sum()} records)\")\n",
    "        print(f\"  Extreme Heat: > {temp_data.quantile(0.95):.1f} ({(temp_data > temp_data.quantile(0.95)).sum()} records)\")\n",
    "    \n",
    "    # Pressure extremes\n",
    "    pressure_cols = [col for col in df.columns if 'pressure' in col.lower()]\n",
    "    if pressure_cols:\n",
    "        pressure_col = pressure_cols[0]\n",
    "        pressure_data = pd.to_numeric(df[pressure_col], errors='coerce')\n",
    "        print(f\"Pressure ({pressure_col}):\")\n",
    "        print(f\"  Very Low: < {pressure_data.quantile(0.05):.1f} ({(pressure_data < pressure_data.quantile(0.05)).sum()} records)\")\n",
    "        print(f\"  Very High: > {pressure_data.quantile(0.95):.1f} ({(pressure_data > pressure_data.quantile(0.95)).sum()} records)\")\n",
    "    \n",
    "    # Wind extremes\n",
    "    wind_cols = [col for col in df.columns if any(x in col.lower() for x in ['wind', 'mph', 'kph'])]\n",
    "    if wind_cols:\n",
    "        wind_col = wind_cols[0]\n",
    "        wind_data = pd.to_numeric(df[wind_col], errors='coerce')\n",
    "        print(f\"Wind ({wind_col}):\")\n",
    "        print(f\"  High Wind: > {wind_data.quantile(0.90):.1f} ({(wind_data > wind_data.quantile(0.90)).sum()} records)\")\n",
    "        print(f\"  Extreme Wind: > {wind_data.quantile(0.95):.1f} ({(wind_data > wind_data.quantile(0.95)).sum()} records)\")\n",
    "    \n",
    "    # Precipitation extremes\n",
    "    precip_cols = [col for col in df.columns if any(x in col.lower() for x in ['precip', 'rain', 'precipitation'])]\n",
    "    if precip_cols:\n",
    "        precip_col = precip_cols[0]\n",
    "        precip_data = pd.to_numeric(df[precip_col], errors='coerce')\n",
    "        print(f\"Precipitation ({precip_col}):\")\n",
    "        print(f\"  Heavy Rain: > {precip_data.quantile(0.90):.1f} ({(precip_data > precip_data.quantile(0.90)).sum()} records)\")\n",
    "        print(f\"  Extreme Rain: > {precip_data.quantile(0.95):.1f} ({(precip_data > precip_data.quantile(0.95)).sum()} records)\")\n",
    "\n",
    "# Analyze extreme conditions for key datasets\n",
    "key_datasets = ['classification', 'rain_prediction', 'global']\n",
    "for name in key_datasets:\n",
    "    if name in datasets:\n",
    "        identify_extreme_conditions(datasets[name], name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for weather classification dataset\n",
    "if 'classification' in datasets:\n",
    "    df_weather = datasets['classification']\n",
    "    numeric_cols = df_weather.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df_weather[numeric_cols].corr()\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "        plt.title('Weather Variables Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find strong correlations\n",
    "        print(\"\\nStrong Correlations (|r| > 0.5):\")\n",
    "        print(\"-\" * 40)\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.5:\n",
    "                    print(f\"{corr_matrix.columns[i]} vs {corr_matrix.columns[j]}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weather Type Distribution (Classification Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weather type distribution\n",
    "if 'classification' in datasets:\n",
    "    df_weather = datasets['classification']\n",
    "    \n",
    "    # Check if weather type column exists\n",
    "    weather_type_cols = [col for col in df_weather.columns if 'weather' in col.lower() or 'type' in col.lower()]\n",
    "    \n",
    "    if weather_type_cols:\n",
    "        weather_col = weather_type_cols[0]\n",
    "        \n",
    "        # Plot weather type distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        weather_counts = df_weather[weather_col].value_counts()\n",
    "        weather_counts.plot(kind='bar', color='lightblue')\n",
    "        plt.title('Weather Type Distribution')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        weather_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Weather Type Percentage')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nWeather Type Counts:\")\n",
    "        print(weather_counts)\n",
    "        \n",
    "        # Analyze weather conditions by type\n",
    "        numeric_cols = df_weather.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"\\nAverage Weather Conditions by Type:\")\n",
    "            print(df_weather.groupby(weather_col)[numeric_cols].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"Assess data quality for a dataset\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATA QUALITY ASSESSMENT: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    print(f\"\\nMissing Values:\")\n",
    "    if missing_pct.sum() == 0:\n",
    "        print(\"  ✓ No missing values\")\n",
    "    else:\n",
    "        print(f\"  ✗ {missing_pct[missing_pct > 0].count()} columns with missing data\")\n",
    "        for col, pct in missing_pct[missing_pct > 0].items():\n",
    "            print(f\"    {col}: {pct:.1f}%\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate Rows: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData Types:\")\n",
    "    for dtype, count in df.dtypes.value_counts().items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Numeric columns analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric Columns Analysis:\")\n",
    "        for col in numeric_cols:\n",
    "            data = df[col]\n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    Range: {data.min():.2f} to {data.max():.2f}\")\n",
    "            print(f\"    Mean: {data.mean():.2f}, Std: {data.std():.2f}\")\n",
    "            \n",
    "            # Check for outliers (values beyond 3 standard deviations)\n",
    "            outliers = np.abs((data - data.mean()) / data.std()) > 3\n",
    "            outlier_count = outliers.sum()\n",
    "            if outlier_count > 0:\n",
    "                print(f\"    ⚠ Outliers: {outlier_count} ({outlier_count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'missing_pct': missing_pct.sum(),\n",
    "        'duplicates': duplicates,\n",
    "        'numeric_cols': len(numeric_cols)\n",
    "    }\n",
    "\n",
    "# Assess quality for all datasets\n",
    "quality_summary = {}\n",
    "for name, df in datasets.items():\n",
    "    quality_summary[name] = assess_data_quality(df, name)\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "summary_df = pd.DataFrame(quality_summary).T\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations for Data Processing\n",
    "\n",
    "Based on the exploration above, here are the key findings and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PROCESSING RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = [\n",
    "    \"1. PRIMARY DATASETS FOR ML TRAINING:\",\n",
    "    \"   • weather_classification_data.csv - Best for labeled weather patterns\",\n",
    "    \"   • rain_prediction_2500observations.csv - Good for binary classification\",\n",
    "    \"   • GlobalWeatherRepository.csv - Rich feature set for current conditions\",\n",
    "    \"\",\n",
    "    \"2. DATA CLEANING PRIORITIES:\",\n",
    "    \"   • Handle missing values using forward fill for time series data\",\n",
    "    \"   • Remove duplicate records to avoid bias\",\n",
    "    \"   • Standardize units (ensure consistent temperature, pressure, wind units)\",\n",
    "    \"   • Validate extreme values and handle outliers appropriately\",\n",
    "    \"\",\n",
    "    \"3. FEATURE ENGINEERING OPPORTUNITIES:\",\n",
    "    \"   • Create rolling window statistics (7-day averages, max, min)\",\n",
    "    \"   • Calculate pressure drop rates and wind speed changes\",\n",
    "    \"   • Engineer composite features (pressure + wind + precipitation)\",\n",
    "    \"   • Create seasonal and location-based features\",\n",
    "    \"\",\n",
    "    \"4. DISASTER LABELING STRATEGY:\",\n",
    "    \"   • Use extreme weather thresholds to create synthetic disaster labels\",\n",
    "    \"   • Combine multiple weather factors for more accurate labeling\",\n",
    "    \"   • Consider regional variations in extreme weather definitions\",\n",
    "    \"\",\n",
    "    \"5. MODEL TRAINING CONSIDERATIONS:\",\n",
    "    \"   • Use stratified sampling to maintain class balance\",\n",
    "    \"   • Consider ensemble methods for better prediction accuracy\",\n",
    "    \"   • Implement cross-validation for robust model evaluation\",\n",
    "    \"   • Monitor for overfitting with limited disaster examples\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATION COMPLETE - Ready for data preprocessing pipeline!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}