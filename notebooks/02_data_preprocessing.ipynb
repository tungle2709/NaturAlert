{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline - Disaster Early Warning System\n",
    "\n",
    "This notebook implements the complete data preprocessing pipeline:\n",
    "1. Load raw weather datasets\n",
    "2. Apply data cleaning and standardization\n",
    "3. Create synthetic disaster labels\n",
    "4. Engineer features for ML training\n",
    "5. Perform train/test split\n",
    "6. Export processed data to SQLite\n",
    "7. Verify data quality and feature distributions\n",
    "\n",
    "**Output**: Clean, labeled, and feature-engineered dataset ready for ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add backend modules to path\n",
    "sys.path.append('../backend')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from backend.utils.data_loader import DataLoader\n",
    "from backend.utils.disaster_labeler import DisasterLabeler\n",
    "from backend.utils.feature_engineer import FeatureEngineer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✓ All modules imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Weather Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(dataset_dir='../dataset')\n",
    "\n",
    "# Load all available datasets\n",
    "print(\"Loading weather datasets...\")\n",
    "datasets = data_loader.load_weather_datasets()\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(\"=\" * 60)\n",
    "dataset_info = data_loader.get_dataset_info()\n",
    "\n",
    "for name, info in dataset_info.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Shape: {info['shape']}\")\n",
    "    print(f\"  Numeric columns: {len(info['numeric_columns'])}\")\n",
    "    print(f\"  Missing values: {info['missing_values']}\")\n",
    "    print(f\"  Memory usage: {info['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"  Key columns: {info['numeric_columns'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select Primary Dataset for Processing\n",
    "\n",
    "Based on the exploration, we'll use the weather classification dataset as our primary source for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select primary dataset for processing\n",
    "primary_datasets = ['classification', 'rain_prediction', 'global']\n",
    "selected_datasets = []\n",
    "\n",
    "for dataset_name in primary_datasets:\n",
    "    if dataset_name in datasets:\n",
    "        selected_datasets.append(dataset_name)\n",
    "        print(f\"✓ Selected {dataset_name}: {datasets[dataset_name].shape}\")\n",
    "\n",
    "if not selected_datasets:\n",
    "    print(\"⚠ No suitable datasets found. Using first available dataset.\")\n",
    "    selected_datasets = [list(datasets.keys())[0]]\n",
    "\n",
    "# Combine selected datasets if multiple are available\n",
    "if len(selected_datasets) > 1:\n",
    "    print(f\"\\nCombining {len(selected_datasets)} datasets...\")\n",
    "    combined_df = data_loader.get_combined_dataset(selected_datasets)\n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "else:\n",
    "    combined_df = datasets[selected_datasets[0]].copy()\n",
    "    print(f\"Using single dataset: {combined_df.shape}\")\n",
    "\n",
    "# Display basic info about combined dataset\n",
    "print(f\"\\nCombined Dataset Info:\")\n",
    "print(f\"Shape: {combined_df.shape}\")\n",
    "print(f\"Columns: {list(combined_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Disaster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize disaster labeler\n",
    "disaster_labeler = DisasterLabeler()\n",
    "\n",
    "# Create disaster labels\n",
    "print(\"Creating synthetic disaster labels...\")\n",
    "labeled_df = disaster_labeler.create_disaster_labels(\n",
    "    combined_df, \n",
    "    region_type='temperate'  # Can be adjusted based on data\n",
    ")\n",
    "\n",
    "# Analyze disaster distribution\n",
    "disaster_stats = {\n",
    "    'total_records': len(labeled_df),\n",
    "    'disaster_records': (labeled_df['disaster_occurred'] == 1).sum(),\n",
    "    'disaster_percentage': (labeled_df['disaster_occurred'] == 1).mean() * 100\n",
    "}\n",
    "\n",
    "print(f\"\\nDisaster Labeling Results:\")\n",
    "print(f\"Total records: {disaster_stats['total_records']:,}\")\n",
    "print(f\"Disaster records: {disaster_stats['disaster_records']:,}\")\n",
    "print(f\"Disaster percentage: {disaster_stats['disaster_percentage']:.2f}%\")\n",
    "\n",
    "# Show disaster type distribution\n",
    "if disaster_stats['disaster_records'] > 0:\n",
    "    disaster_types = labeled_df[labeled_df['disaster_occurred'] == 1]['disaster_type'].value_counts()\n",
    "    print(f\"\\nDisaster Type Distribution:\")\n",
    "    for disaster_type, count in disaster_types.items():\n",
    "        percentage = (count / disaster_stats['disaster_records']) * 100\n",
    "        print(f\"  {disaster_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Plot disaster distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Disaster occurrence pie chart\n",
    "    disaster_counts = labeled_df['disaster_occurred'].value_counts()\n",
    "    ax1.pie(disaster_counts.values, labels=['No Disaster', 'Disaster'], autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Disaster Occurrence Distribution')\n",
    "    \n",
    "    # Disaster type bar chart\n",
    "    disaster_types.plot(kind='bar', ax=ax2, color='coral')\n",
    "    ax2.set_title('Disaster Type Distribution')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠ No disasters identified with current thresholds. Consider adjusting criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engineer Features for ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer(window_size=7)\n",
    "\n",
    "# Engineer features\n",
    "print(\"Engineering features for ML training...\")\n",
    "featured_df = feature_engineer.engineer_features(\n",
    "    labeled_df,\n",
    "    datetime_col=None,  # Add if datetime column exists\n",
    "    location_col='source_dataset' if 'source_dataset' in labeled_df.columns else None\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature Engineering Results:\")\n",
    "print(f\"Original columns: {len(labeled_df.columns)}\")\n",
    "print(f\"After feature engineering: {len(featured_df.columns)}\")\n",
    "\n",
    "# Get list of engineered features\n",
    "original_cols = set(labeled_df.columns)\n",
    "new_cols = set(featured_df.columns) - original_cols\n",
    "engineered_features = list(new_cols)\n",
    "\n",
    "print(f\"\\nEngineered Features ({len(engineered_features)}):\")\n",
    "for i, feature in enumerate(engineered_features, 1):\n",
    "    if feature in featured_df.columns:\n",
    "        mean_val = featured_df[feature].mean()\n",
    "        std_val = featured_df[feature].std()\n",
    "        print(f\"{i:2d}. {feature}: {mean_val:.3f} ± {std_val:.3f}\")\n",
    "\n",
    "# Create feature summary\n",
    "feature_summary = feature_engineer.create_feature_summary(featured_df)\n",
    "print(f\"\\nFeature Summary Statistics:\")\n",
    "print(f\"Total engineered features: {feature_summary['total_features']}\")\n",
    "print(f\"Features with valid statistics: {len(feature_summary['feature_statistics'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance if we have disaster labels\n",
    "if 'disaster_occurred' in featured_df.columns and featured_df['disaster_occurred'].sum() > 0:\n",
    "    print(\"Calculating feature importance...\")\n",
    "    \n",
    "    # Get feature importance ranking\n",
    "    importance_df = feature_engineer.get_feature_importance_ranking(\n",
    "        featured_df, target_col='disaster_occurred'\n",
    "    )\n",
    "    \n",
    "    if not importance_df.empty:\n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        print(\"=\" * 60)\n",
    "        top_features = importance_df.head(10)\n",
    "        for idx, row in top_features.iterrows():\n",
    "            print(f\"{row['feature']:25s}: {row['correlation']:.4f}\")\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_10 = importance_df.head(10)\n",
    "        plt.barh(range(len(top_10)), top_10['correlation'], color='skyblue')\n",
    "        plt.yticks(range(len(top_10)), top_10['feature'])\n",
    "        plt.xlabel('Absolute Correlation with Disaster Occurrence')\n",
    "        plt.title('Top 10 Feature Importance (Correlation with Disasters)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠ Could not calculate feature importance\")\n",
    "else:\n",
    "    print(\"⚠ Skipping feature importance analysis (no disaster labels found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for ML training\n",
    "print(\"Preparing data for ML training...\")\n",
    "\n",
    "# Define feature columns (engineered + original weather variables)\n",
    "weather_features = ['temperature', 'pressure', 'wind_speed', 'precipitation', 'humidity']\n",
    "engineered_feature_names = [\n",
    "    'pressure_drop_7d', 'wind_spike_max', 'rain_accumulation_7d',\n",
    "    'humidity_trend', 'temp_deviation', 'pressure_velocity', 'wind_gust_ratio'\n",
    "]\n",
    "\n",
    "# Select available features\n",
    "available_features = []\n",
    "for feature in weather_features + engineered_feature_names:\n",
    "    if feature in featured_df.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Available features for ML: {len(available_features)}\")\n",
    "print(f\"Features: {available_features}\")\n",
    "\n",
    "# Prepare feature matrix and target vector\n",
    "if available_features and 'disaster_occurred' in featured_df.columns:\n",
    "    X = featured_df[available_features].copy()\n",
    "    y = featured_df['disaster_occurred'].copy()\n",
    "    \n",
    "    # Handle missing values in features\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "    print(f\"Positive class ratio: {y.mean():.4f}\")\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(f\"\\nData Quality Check:\")\n",
    "    print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "    print(f\"Missing values in y: {y.isnull().sum()}\")\n",
    "    print(f\"Infinite values in X: {np.isinf(X).sum().sum()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ Cannot prepare ML data - missing features or target column\")\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified train/test split\n",
    "if X is not None and y is not None:\n",
    "    print(\"Performing train/test split...\")\n",
    "    \n",
    "    # Check if we have enough positive samples for stratification\n",
    "    positive_samples = y.sum()\n",
    "    \n",
    "    if positive_samples >= 2:  # Need at least 2 positive samples for stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "        print(\"✓ Stratified split completed\")\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        print(\"✓ Regular split completed (insufficient positive samples for stratification)\")\n",
    "    \n",
    "    print(f\"\\nSplit Results:\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples ({y_train.mean():.4f} positive rate)\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean():.4f} positive rate)\")\n",
    "    \n",
    "    # Feature scaling (optional, but recommended for some algorithms)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"✓ Feature scaling completed\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ Skipping train/test split - no valid data available\")\n",
    "    X_train = X_test = y_train = y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Processed Data to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data to SQLite database\n",
    "print(\"Exporting processed data to SQLite...\")\n",
    "\n",
    "# Database file path\n",
    "db_path = '../disaster_data.db'\n",
    "\n",
    "try:\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Export full processed dataset\n",
    "    featured_df.to_sql('processed_weather_data', conn, if_exists='replace', index=False)\n",
    "    print(f\"✓ Exported full dataset: {len(featured_df)} records\")\n",
    "    \n",
    "    # Export training data if available\n",
    "    if X_train is not None:\n",
    "        # Combine features and target for training set\n",
    "        train_df = pd.DataFrame(X_train, columns=available_features)\n",
    "        train_df['disaster_occurred'] = y_train.values\n",
    "        train_df.to_sql('training_data', conn, if_exists='replace', index=False)\n",
    "        print(f\"✓ Exported training data: {len(train_df)} records\")\n",
    "        \n",
    "        # Export test data\n",
    "        test_df = pd.DataFrame(X_test, columns=available_features)\n",
    "        test_df['disaster_occurred'] = y_test.values\n",
    "        test_df.to_sql('test_data', conn, if_exists='replace', index=False)\n",
    "        print(f\"✓ Exported test data: {len(test_df)} records\")\n",
    "    \n",
    "    # Export feature metadata\n",
    "    feature_metadata = []\n",
    "    for feature in available_features:\n",
    "        if feature in featured_df.columns:\n",
    "            feature_metadata.append({\n",
    "                'feature_name': feature,\n",
    "                'mean': featured_df[feature].mean(),\n",
    "                'std': featured_df[feature].std(),\n",
    "                'min': featured_df[feature].min(),\n",
    "                'max': featured_df[feature].max(),\n",
    "                'description': feature_engineer.feature_definitions.get(feature, 'Weather variable')\n",
    "            })\n",
    "    \n",
    "    feature_metadata_df = pd.DataFrame(feature_metadata)\n",
    "    feature_metadata_df.to_sql('feature_metadata', conn, if_exists='replace', index=False)\n",
    "    print(f\"✓ Exported feature metadata: {len(feature_metadata_df)} features\")\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n✓ All data exported successfully to {db_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error exporting to SQLite: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Quality Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data quality and feature distributions\n",
    "print(\"Verifying data quality and feature distributions...\")\n",
    "\n",
    "# Overall data quality metrics\n",
    "quality_metrics = {\n",
    "    'total_records': len(featured_df),\n",
    "    'total_features': len(featured_df.columns),\n",
    "    'engineered_features': len(engineered_features),\n",
    "    'disaster_records': featured_df['disaster_occurred'].sum() if 'disaster_occurred' in featured_df.columns else 0,\n",
    "    'missing_values': featured_df.isnull().sum().sum(),\n",
    "    'duplicate_records': featured_df.duplicated().sum()\n",
    "}\n",
    "\n",
    "print(f\"\\nData Quality Metrics:\")\n",
    "print(f\"{'='*50}\")\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"{metric.replace('_', ' ').title():25s}: {value:,}\")\n",
    "\n",
    "# Feature distribution analysis\n",
    "if available_features:\n",
    "    print(f\"\\nFeature Distribution Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Plot feature distributions\n",
    "    n_features = min(6, len(available_features))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(available_features[:n_features]):\n",
    "        if feature in featured_df.columns:\n",
    "            featured_df[feature].hist(bins=30, ax=axes[i], alpha=0.7, color='lightblue')\n",
    "            axes[i].set_title(f'{feature} Distribution')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_val = featured_df[feature].mean()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7)\n",
    "            axes[i].text(0.7, 0.9, f'Mean: {mean_val:.2f}', transform=axes[i].transAxes,\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation matrix for key features\n",
    "if len(available_features) > 1:\n",
    "    print(f\"\\nFeature Correlation Analysis:\")\n",
    "    key_features = available_features[:8]  # Limit to 8 features for readability\n",
    "    corr_matrix = featured_df[key_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "               square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nHighly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No highly correlated features found (good for ML)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipeline Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pipeline summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_stats = {\n",
    "    'Input Datasets': len(datasets),\n",
    "    'Selected Datasets': len(selected_datasets),\n",
    "    'Total Records Processed': len(featured_df),\n",
    "    'Original Features': len(labeled_df.columns),\n",
    "    'Engineered Features': len(engineered_features),\n",
    "    'Final Feature Count': len(available_features),\n",
    "    'Disaster Records': quality_metrics['disaster_records'],\n",
    "    'Disaster Rate': f\"{(quality_metrics['disaster_records'] / quality_metrics['total_records'] * 100):.2f}%\" if quality_metrics['total_records'] > 0 else \"0%\",\n",
    "    'Training Samples': len(X_train) if X_train is not None else 0,\n",
    "    'Test Samples': len(X_test) if X_test is not None else 0\n",
    "}\n",
    "\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pipeline_status = [\n",
    "    (\"✓\", \"Data Loading\", \"Successfully loaded weather datasets\"),\n",
    "    (\"✓\", \"Data Cleaning\", \"Applied standardization and missing value handling\"),\n",
    "    (\"✓\", \"Disaster Labeling\", f\"Created {quality_metrics['disaster_records']} disaster labels\"),\n",
    "    (\"✓\", \"Feature Engineering\", f\"Generated {len(engineered_features)} engineered features\"),\n",
    "    (\"✓\" if X_train is not None else \"⚠\", \"Train/Test Split\", \"Prepared data for ML training\" if X_train is not None else \"Insufficient data for ML\"),\n",
    "    (\"✓\", \"Data Export\", \"Exported processed data to SQLite database\"),\n",
    "    (\"✓\", \"Quality Verification\", \"Verified data quality and feature distributions\")\n",
    "]\n",
    "\n",
    "for status, step, description in pipeline_status:\n",
    "    print(f\"{status} {step:20s}: {description}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "next_steps = [\n",
    "    \"1. MACHINE LEARNING MODEL TRAINING:\",\n",
    "    \"   • Train binary classification model (disaster vs no disaster)\",\n",
    "    \"   • Train multi-class model (disaster type classification)\",\n",
    "    \"   • Evaluate model performance with cross-validation\",\n",
    "    \"   • Save trained models for deployment\",\n",
    "    \"\",\n",
    "    \"2. MODEL EVALUATION:\",\n",
    "    \"   • Calculate accuracy, precision, recall, F1-score\",\n",
    "    \"   • Generate confusion matrices\",\n",
    "    \"   • Analyze feature importance\",\n",
    "    \"   • Test model on holdout data\",\n",
    "    \"\",\n",
    "    \"3. BACKEND INTEGRATION:\",\n",
    "    \"   • Implement prediction engine\",\n",
    "    \"   • Create API endpoints\",\n",
    "    \"   • Integrate with Gemini AI for explanations\",\n",
    "    \"   • Set up real-time prediction pipeline\",\n",
    "    \"\",\n",
    "    \"4. FRONTEND DEVELOPMENT:\",\n",
    "    \"   • Build React dashboard\",\n",
    "    \"   • Create visualization components\",\n",
    "    \"   • Implement user interface\",\n",
    "    \"   • Add real-time updates\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPROCESSING PIPELINE COMPLETE!\")\n",
    "print(\"Ready for ML model training and system development.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}