{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Training - Disaster Early Warning System\n",
    "\n",
    "This notebook implements the complete ML model training pipeline:\n",
    "1. Load processed features from SQLite\n",
    "2. Split data into train/validation/test sets\n",
    "3. Train binary classification model (disaster vs no disaster)\n",
    "4. Train multi-class model (disaster type classification)\n",
    "5. Evaluate model performance with comprehensive metrics\n",
    "6. Generate feature importance analysis\n",
    "7. Save trained models as .pkl files\n",
    "\n",
    "**Output**: Trained ML models ready for deployment in the prediction engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add backend modules to path\n",
    "sys.path.append('../backend')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✓ All modules imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Features from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database file path\n",
    "db_path = '../disaster_data.db'\n",
    "\n",
    "print(\"Loading processed data from SQLite...\")\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load training data\n",
    "training_df = pd.read_sql_query(\"SELECT * FROM training_data\", conn)\n",
    "print(f\"✓ Loaded training data: {training_df.shape}\")\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_sql_query(\"SELECT * FROM test_data\", conn)\n",
    "print(f\"✓ Loaded test data: {test_df.shape}\")\n",
    "\n",
    "# Load feature metadata\n",
    "feature_metadata_df = pd.read_sql_query(\"SELECT * FROM feature_metadata\", conn)\n",
    "print(f\"✓ Loaded feature metadata: {feature_metadata_df.shape}\")\n",
    "\n",
    "# Load full processed dataset for additional analysis\n",
    "full_df = pd.read_sql_query(\"SELECT * FROM processed_weather_data\", conn)\n",
    "print(f\"✓ Loaded full processed dataset: {full_df.shape}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Training samples: {len(training_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Features: {len(feature_metadata_df)}\")\n",
    "print(f\"Training disaster rate: {training_df['disaster_occurred'].mean():.4f}\")\n",
    "print(f\"Test disaster rate: {test_df['disaster_occurred'].mean():.4f}\")\n",
    "\n",
    "# Display feature information\n",
    "print(f\"\\nFeature Information:\")\n",
    "print(feature_metadata_df[['feature_name', 'mean', 'std', 'description']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names (exclude target variable)\n",
    "feature_columns = [col for col in training_df.columns if col != 'disaster_occurred']\n",
    "target_column = 'disaster_occurred'\n",
    "\n",
    "print(f\"Feature columns ({len(feature_columns)}): {feature_columns}\")\n",
    "\n",
    "# Prepare training data\n",
    "X_train = training_df[feature_columns].copy()\n",
    "y_train = training_df[target_column].copy()\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_df[feature_columns].copy()\n",
    "y_test = test_df[target_column].copy()\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Features shape: {X_train.shape}\")\n",
    "print(f\"  Target shape: {y_train.shape}\")\n",
    "print(f\"  Positive class ratio: {y_train.mean():.4f}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Features shape: {X_test.shape}\")\n",
    "print(f\"  Target shape: {y_test.shape}\")\n",
    "print(f\"  Positive class ratio: {y_test.mean():.4f}\")\n",
    "\n",
    "# Check for data quality issues\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"Missing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y_train: {y_train.isnull().sum()}\")\n",
    "print(f\"Missing values in y_test: {y_test.isnull().sum()}\")\n",
    "print(f\"Infinite values in X_train: {np.isinf(X_train).sum().sum()}\")\n",
    "print(f\"Infinite values in X_test: {np.isinf(X_test).sum().sum()}\")\n",
    "\n",
    "# Feature scaling (important for some algorithms)\n",
    "print(f\"\\nApplying feature scaling...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✓ Feature scaling completed\")\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Data into Train/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split training data into train/validation\n",
    "print(\"Creating train/validation split...\")\n",
    "\n",
    "# Check if we have enough positive samples for stratification\n",
    "positive_samples = y_train.sum()\n",
    "print(f\"Positive samples in training set: {positive_samples}\")\n",
    "\n",
    "if positive_samples >= 4:  # Need at least 4 for stratified split (2 for train, 2 for val)\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "    \n",
    "    X_train_final_scaled, X_val_scaled, _, _ = train_test_split(\n",
    "        X_train_scaled, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "    print(\"✓ Stratified train/validation split completed\")\n",
    "else:\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train_final_scaled, X_val_scaled, _, _ = train_test_split(\n",
    "        X_train_scaled, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(\"✓ Regular train/validation split completed (insufficient positive samples for stratification)\")\n",
    "\n",
    "print(f\"\\nFinal Data Split:\")\n",
    "print(f\"Training set: {X_train_final.shape[0]:,} samples ({y_train_final.mean():.4f} positive rate)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({y_val.mean():.4f} positive rate)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({y_test.mean():.4f} positive rate)\")\n",
    "\n",
    "# Create a summary of the data splits\n",
    "split_summary = pd.DataFrame({\n",
    "    'Dataset': ['Training', 'Validation', 'Test'],\n",
    "    'Samples': [len(X_train_final), len(X_val), len(X_test)],\n",
    "    'Positive_Samples': [y_train_final.sum(), y_val.sum(), y_test.sum()],\n",
    "    'Positive_Rate': [y_train_final.mean(), y_val.mean(), y_test.mean()]\n",
    "})\n",
    "\n",
    "print(f\"\\nData Split Summary:\")\n",
    "print(split_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Binary Classification Model (Disaster vs No Disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training binary classification model (Random Forest)...\")\n",
    "\n",
    "# Initialize Random Forest classifier with class balancing\n",
    "rf_binary = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_binary.fit(X_train_final, y_train_final)\n",
    "print(\"✓ Random Forest binary classifier trained\")\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_val_pred = rf_binary.predict(X_val)\n",
    "y_val_pred_proba = rf_binary.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Make predictions on test set\n",
    "y_test_pred = rf_binary.predict(X_test)\n",
    "y_test_pred_proba = rf_binary.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✓ Predictions generated for validation and test sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing hyperparameter tuning with cross-validation...\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use stratified k-fold cross-validation if we have enough positive samples\n",
    "if positive_samples >= 10:  # Need at least 10 positive samples for 5-fold CV\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    print(\"Using stratified 5-fold cross-validation\")\n",
    "else:\n",
    "    cv_strategy = 3  # Use simple 3-fold CV\n",
    "    print(\"Using simple 3-fold cross-validation (insufficient positive samples for stratified CV)\")\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "# Note: Using a smaller parameter grid due to class imbalance\n",
    "param_grid_small = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_grid_small,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',  # Use F1 score for imbalanced data\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_final, y_train_final)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "rf_binary_best = grid_search.best_estimator_\n",
    "print(\"✓ Hyperparameter tuning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating binary classification model...\")\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_val_pred_best = rf_binary_best.predict(X_val)\n",
    "y_val_pred_proba_best = rf_binary_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "y_test_pred_best = rf_binary_best.predict(X_test)\n",
    "y_test_pred_proba_best = rf_binary_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "val_metrics = {\n",
    "    'accuracy': accuracy_score(y_val, y_val_pred_best),\n",
    "    'precision': precision_score(y_val, y_val_pred_best, zero_division=0),\n",
    "    'recall': recall_score(y_val, y_val_pred_best, zero_division=0),\n",
    "    'f1': f1_score(y_val, y_val_pred_best, zero_division=0)\n",
    "}\n",
    "\n",
    "# Calculate ROC-AUC if we have positive samples in validation set\n",
    "if y_val.sum() > 0 and len(np.unique(y_val)) > 1:\n",
    "    val_metrics['roc_auc'] = roc_auc_score(y_val, y_val_pred_proba_best)\n",
    "else:\n",
    "    val_metrics['roc_auc'] = 0.0\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_test_pred_best),\n",
    "    'precision': precision_score(y_test, y_test_pred_best, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_test_pred_best, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_test_pred_best, zero_division=0)\n",
    "}\n",
    "\n",
    "# Calculate ROC-AUC if we have positive samples in test set\n",
    "if y_test.sum() > 0 and len(np.unique(y_test)) > 1:\n",
    "    test_metrics['roc_auc'] = roc_auc_score(y_test, y_test_pred_proba_best)\n",
    "else:\n",
    "    test_metrics['roc_auc'] = 0.0\n",
    "\n",
    "print(f\"\\nValidation Set Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "\n",
    "# Create metrics comparison DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': list(val_metrics.keys()),\n",
    "    'Validation': list(val_metrics.values()),\n",
    "    'Test': list(test_metrics.values())\n",
    "})\n",
    "\n",
    "print(f\"\\nMetrics Comparison:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating confusion matrix and classification report...\")\n",
    "\n",
    "# Generate confusion matrices\n",
    "cm_val = confusion_matrix(y_val, y_val_pred_best)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred_best)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Validation confusion matrix\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "           xticklabels=['No Disaster', 'Disaster'],\n",
    "           yticklabels=['No Disaster', 'Disaster'])\n",
    "ax1.set_title('Validation Set Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Test confusion matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "           xticklabels=['No Disaster', 'Disaster'],\n",
    "           yticklabels=['No Disaster', 'Disaster'])\n",
    "ax2.set_title('Test Set Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate classification reports\n",
    "print(f\"\\nValidation Set Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_best, target_names=['No Disaster', 'Disaster'], zero_division=0))\n",
    "\n",
    "print(f\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_best, target_names=['No Disaster', 'Disaster'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance from the trained model\n",
    "feature_importance = rf_binary_best.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance Ranking:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Feature Importance - Binary Classification Model')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate cumulative importance\n",
    "importance_df['cumulative_importance'] = importance_df['importance'].cumsum()\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "top_features = importance_df.head(5)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"  {row['feature']:20s}: {row['importance']:.4f} ({row['cumulative_importance']:.4f} cumulative)\")\n",
    "\n",
    "# Identify features contributing to 80% of importance\n",
    "important_features_80 = importance_df[importance_df['cumulative_importance'] <= 0.8]['feature'].tolist()\n",
    "print(f\"\\nFeatures contributing to 80% of importance ({len(important_features_80)}): {important_features_80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Disaster Type Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training disaster type classification model...\")\n",
    "\n",
    "# Check if we have disaster type information in the full dataset\n",
    "if 'disaster_type' in full_df.columns:\n",
    "    # Filter data to only disaster events\n",
    "    disaster_data = full_df[full_df['disaster_occurred'] == 1].copy()\n",
    "    \n",
    "    if len(disaster_data) > 0:\n",
    "        print(f\"Found {len(disaster_data)} disaster events for type classification\")\n",
    "        \n",
    "        # Check disaster type distribution\n",
    "        disaster_type_counts = disaster_data['disaster_type'].value_counts()\n",
    "        print(f\"\\nDisaster Type Distribution:\")\n",
    "        print(disaster_type_counts)\n",
    "        \n",
    "        # Only proceed if we have multiple disaster types and sufficient samples\n",
    "        if len(disaster_type_counts) > 1 and len(disaster_data) >= 10:\n",
    "            # Prepare features and target for disaster type classification\n",
    "            X_disaster = disaster_data[feature_columns].copy()\n",
    "            y_disaster_type = disaster_data['disaster_type'].copy()\n",
    "            \n",
    "            # Handle missing values\n",
    "            X_disaster = X_disaster.fillna(X_disaster.median())\n",
    "            \n",
    "            # Split disaster data into train/test\n",
    "            if len(disaster_data) >= 20:  # Need sufficient samples for split\n",
    "                X_disaster_train, X_disaster_test, y_disaster_train, y_disaster_test = train_test_split(\n",
    "                    X_disaster, y_disaster_type,\n",
    "                    test_size=0.3,\n",
    "                    random_state=42,\n",
    "                    stratify=y_disaster_type if len(disaster_type_counts) > 1 else None\n",
    "                )\n",
    "            else:\n",
    "                # Use all data for training if sample size is small\n",
    "                X_disaster_train = X_disaster\n",
    "                y_disaster_train = y_disaster_type\n",
    "                X_disaster_test = X_disaster\n",
    "                y_disaster_test = y_disaster_type\n",
    "            \n",
    "            print(f\"Disaster type training set: {X_disaster_train.shape[0]} samples\")\n",
    "            print(f\"Disaster type test set: {X_disaster_test.shape[0]} samples\")\n",
    "            \n",
    "            # Train Gradient Boosting classifier for disaster type prediction\n",
    "            gb_multiclass = GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            gb_multiclass.fit(X_disaster_train, y_disaster_train)\n",
    "            print(\"✓ Gradient Boosting disaster type classifier trained\")\n",
    "            \n",
    "            # Make predictions\n",
    "            y_disaster_pred = gb_multiclass.predict(X_disaster_test)\n",
    "            y_disaster_pred_proba = gb_multiclass.predict_proba(X_disaster_test)\n",
    "            \n",
    "            # Evaluate multi-class model\n",
    "            disaster_type_accuracy = accuracy_score(y_disaster_test, y_disaster_pred)\n",
    "            \n",
    "            print(f\"\\nDisaster Type Classification Results:\")\n",
    "            print(f\"Accuracy: {disaster_type_accuracy:.4f}\")\n",
    "            \n",
    "            # Generate classification report for disaster types\n",
    "            print(f\"\\nDisaster Type Classification Report:\")\n",
    "            print(classification_report(y_disaster_test, y_disaster_pred, zero_division=0))\n",
    "            \n",
    "            # Feature importance for disaster type model\n",
    "            disaster_type_importance = gb_multiclass.feature_importances_\n",
    "            disaster_type_importance_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': disaster_type_importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nFeature Importance for Disaster Type Classification:\")\n",
    "            print(disaster_type_importance_df.head(5).to_string(index=False))\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠ Insufficient disaster type diversity or samples for multi-class classification\")\n",
    "            gb_multiclass = None\n",
    "    else:\n",
    "        print(\"⚠ No disaster events found for type classification\")\n",
    "        gb_multiclass = None\n",
    "else:\n",
    "    print(\"⚠ No disaster type column found in dataset\")\n",
    "    gb_multiclass = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model performance visualizations...\")\n",
    "\n",
    "# Create comprehensive performance plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. ROC Curve (if we have positive samples)\n",
    "if y_test.sum() > 0 and len(np.unique(y_test)) > 1:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba_best)\n",
    "    axes[0, 0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                   label=f'ROC curve (AUC = {test_metrics[\"roc_auc\"]:.3f})')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axes[0, 0].set_xlim([0.0, 1.0])\n",
    "    axes[0, 0].set_ylim([0.0, 1.05])\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title('ROC Curve - Binary Classification')\n",
    "    axes[0, 0].legend(loc=\"lower right\")\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'ROC Curve\\nNot Available\\n(No positive samples)', \n",
    "                   ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "    axes[0, 0].set_title('ROC Curve - Binary Classification')\n",
    "\n",
    "# 2. Precision-Recall Curve (if we have positive samples)\n",
    "if y_test.sum() > 0 and len(np.unique(y_test)) > 1:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_pred_proba_best)\n",
    "    axes[0, 1].plot(recall, precision, color='blue', lw=2)\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "    axes[0, 1].set_xlim([0.0, 1.0])\n",
    "    axes[0, 1].set_ylim([0.0, 1.05])\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Precision-Recall Curve\\nNot Available\\n(No positive samples)', \n",
    "                   ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "\n",
    "# 3. Feature Importance (Top 10)\n",
    "top_10_features = importance_df.head(10)\n",
    "axes[1, 0].barh(range(len(top_10_features)), top_10_features['importance'], color='skyblue')\n",
    "axes[1, 0].set_yticks(range(len(top_10_features)))\n",
    "axes[1, 0].set_yticklabels(top_10_features['feature'])\n",
    "axes[1, 0].set_xlabel('Importance Score')\n",
    "axes[1, 0].set_title('Top 10 Feature Importance')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Model Performance Comparison\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "val_scores = [val_metrics['accuracy'], val_metrics['precision'], val_metrics['recall'], val_metrics['f1']]\n",
    "test_scores = [test_metrics['accuracy'], test_metrics['precision'], test_metrics['recall'], test_metrics['f1']]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, val_scores, width, label='Validation', color='lightcoral')\n",
    "axes[1, 1].bar(x + width/2, test_scores, width, label='Test', color='lightblue')\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Model Performance Comparison')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics_names)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Performance visualizations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving trained models...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save binary classification model\n",
    "binary_model_path = models_dir / 'disaster_prediction_model.pkl'\n",
    "joblib.dump(rf_binary_best, binary_model_path)\n",
    "print(f\"✓ Binary classification model saved to: {binary_model_path}\")\n",
    "\n",
    "# Save feature scaler\n",
    "scaler_path = models_dir / 'feature_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Feature scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save disaster type classification model (if available)\n",
    "if gb_multiclass is not None:\n",
    "    multiclass_model_path = models_dir / 'disaster_type_model.pkl'\n",
    "    joblib.dump(gb_multiclass, multiclass_model_path)\n",
    "    print(f\"✓ Disaster type classification model saved to: {multiclass_model_path}\")\n",
    "else:\n",
    "    print(\"⚠ Disaster type classification model not available for saving\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'binary_model': {\n",
    "        'model_type': 'RandomForestClassifier',\n",
    "        'parameters': rf_binary_best.get_params(),\n",
    "        'features': feature_columns,\n",
    "        'performance': test_metrics,\n",
    "        'feature_importance': importance_df.to_dict('records'),\n",
    "        'training_samples': len(X_train_final),\n",
    "        'test_samples': len(X_test),\n",
    "        'positive_class_ratio': y_train_final.mean()\n",
    "    },\n",
    "    'feature_scaler': {\n",
    "        'scaler_type': 'StandardScaler',\n",
    "        'feature_means': scaler.mean_.tolist(),\n",
    "        'feature_scales': scaler.scale_.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "if gb_multiclass is not None:\n",
    "    model_metadata['multiclass_model'] = {\n",
    "        'model_type': 'GradientBoostingClassifier',\n",
    "        'parameters': gb_multiclass.get_params(),\n",
    "        'classes': gb_multiclass.classes_.tolist(),\n",
    "        'training_samples': len(X_disaster_train),\n",
    "        'accuracy': disaster_type_accuracy\n",
    "    }\n",
    "\n",
    "# Save metadata as JSON\n",
    "import json\n",
    "metadata_path = models_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"✓ Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n✓ All models and metadata saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive training summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MACHINE LEARNING MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_summary = {\n",
    "    'Dataset Information': {\n",
    "        'Total Samples': len(full_df),\n",
    "        'Training Samples': len(X_train_final),\n",
    "        'Validation Samples': len(X_val),\n",
    "        'Test Samples': len(X_test),\n",
    "        'Features Used': len(feature_columns),\n",
    "        'Disaster Rate': f\"{y_train_final.mean():.4f}\"\n",
    "    },\n",
    "    'Binary Classification Model': {\n",
    "        'Algorithm': 'Random Forest',\n",
    "        'Best Parameters': rf_binary_best.get_params(),\n",
    "        'Test Accuracy': f\"{test_metrics['accuracy']:.4f}\",\n",
    "        'Test Precision': f\"{test_metrics['precision']:.4f}\",\n",
    "        'Test Recall': f\"{test_metrics['recall']:.4f}\",\n",
    "        'Test F1-Score': f\"{test_metrics['f1']:.4f}\",\n",
    "        'Test ROC-AUC': f\"{test_metrics['roc_auc']:.4f}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "if gb_multiclass is not None:\n",
    "    training_summary['Disaster Type Classification Model'] = {\n",
    "        'Algorithm': 'Gradient Boosting',\n",
    "        'Classes': gb_multiclass.classes_.tolist(),\n",
    "        'Accuracy': f\"{disaster_type_accuracy:.4f}\",\n",
    "        'Training Samples': len(X_disaster_train)\n",
    "    }\n",
    "\n",
    "for section, details in training_summary.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    for key, value in details.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"  {key}: {str(value)[:100]}...\" if len(str(value)) > 100 else f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_status = [\n",
    "    (\"✓\", \"Data Loading\", \"Successfully loaded processed features from SQLite\"),\n",
    "    (\"✓\", \"Data Splitting\", \"Created train/validation/test splits with proper stratification\"),\n",
    "    (\"✓\", \"Binary Model Training\", \"Trained Random Forest classifier with hyperparameter tuning\"),\n",
    "    (\"✓\" if gb_multiclass is not None else \"⚠\", \"Multi-class Model Training\", \n",
    "     \"Trained Gradient Boosting classifier for disaster types\" if gb_multiclass is not None else \"Insufficient data for disaster type classification\"),\n",
    "    (\"✓\", \"Model Evaluation\", \"Comprehensive evaluation with multiple metrics\"),\n",
    "    (\"✓\", \"Feature Analysis\", \"Generated feature importance rankings\"),\n",
    "    (\"✓\", \"Model Persistence\", \"Saved trained models and metadata to disk\")\n",
    "]\n",
    "\n",
    "for status, step, description in training_status:\n",
    "    print(f\"{status} {step:25s}: {description}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "key_findings = [\n",
    "    f\"• Dataset contains {len(full_df):,} weather records with {y_train_final.sum()} disaster events\",\n",
    "    f\"• Class imbalance ratio: {(1-y_train_final.mean())/y_train_final.mean():.0f}:1 (normal:disaster)\",\n",
    "    f\"• Best performing model: Random Forest with {test_metrics['f1']:.3f} F1-score\",\n",
    "    f\"• Top 3 most important features: {', '.join(importance_df.head(3)['feature'].tolist())}\",\n",
    "    f\"• Model achieves {test_metrics['accuracy']:.1%} accuracy on test set\",\n",
    "    f\"• Feature engineering contributed {len([f for f in feature_columns if '_' in f])} engineered features\"\n",
    "]\n",
    "\n",
    "for finding in key_findings:\n",
    "    print(finding)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "next_steps = [\n",
    "    \"1. BACKEND INTEGRATION:\",\n",
    "    \"   • Implement prediction engine to load and use trained models\",\n",
    "    \"   • Create API endpoints for real-time disaster prediction\",\n",
    "    \"   • Integrate with Gemini AI for explanation generation\",\n",
    "    \"   • Set up automated model retraining pipeline\",\n",
    "    \"\",\n",
    "    \"2. MODEL DEPLOYMENT:\",\n",
    "    \"   • Deploy models to production environment\",\n",
    "    \"   • Implement model versioning and rollback capabilities\",\n",
    "    \"   • Set up monitoring for model performance drift\",\n",
    "    \"   • Create model update and retraining workflows\",\n",
    "    \"\",\n",
    "    \"3. PERFORMANCE OPTIMIZATION:\",\n",
    "    \"   • Collect more disaster event data to improve model performance\",\n",
    "    \"   • Experiment with advanced algorithms (XGBoost, Neural Networks)\",\n",
    "    \"   • Implement ensemble methods for better predictions\",\n",
    "    \"   • Optimize feature selection and engineering\",\n",
    "    \"\",\n",
    "    \"4. SYSTEM INTEGRATION:\",\n",
    "    \"   • Connect models to real-time weather data feeds\",\n",
    "    \"   • Implement automated alert generation system\",\n",
    "    \"   • Create dashboard for model performance monitoring\",\n",
    "    \"   • Set up A/B testing for model improvements\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"MACHINE LEARNING MODEL TRAINING COMPLETE!\")\n",
    "print(\"Models are ready for deployment in the disaster prediction system.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": \"python\",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}